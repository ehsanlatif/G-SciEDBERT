{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0lTwJkEZzlE"
   },
   "source": [
    "# MultiClass Classification in 10 Minutes with BERT-TensorFlow and SoftMax\n",
    "- Based on Article  \n",
    "  https://towardsdatascience.com/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaywxYQ7Owcs"
   },
   "source": [
    "- Data Source:\n",
    "  - Unzip files (only one time after downloading tar.gz file)  \n",
    "  http://qwone.com/~jason/20Newsgroups/\n",
    "\n",
    "  - Download Link:  \n",
    "    http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07aNXeymDgN6"
   },
   "source": [
    "## Install Transformers Python Library to run it in CoLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mpyyhRtWZxzD",
    "outputId": "9fd59259-a67d-489f-abde-89fa54b58257"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\n",
    "import pickle\n",
    "from transformers import *\n",
    "from tqdm import tqdm, trange\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19qhcXnnDvbu"
   },
   "source": [
    "## Mount Google Drive to Read Data & Model from Local Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4wlpTiHEUbLy",
    "outputId": "e5957b80-576c-400f-dff0-77728ec7ad0d"
   },
   "outputs": [],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "#if device_name != '/device:GPU:0':\n",
    "#  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "mz2Fbv26sc9T",
    "outputId": "c4e49b2e-523c-4ff8-a492-c344119d6135"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Pbjz0hNU920"
   },
   "outputs": [],
   "source": [
    "dataset_name = 'S327Q02'\n",
    "#sub_dataset = 'gender2'\n",
    "train_data_file = '../dataset/extracted_files/'+dataset_name+'_train.csv'\n",
    "output_model_name = '../models/G-SciEdBERT_model_'+dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ZeObzsGqsrnz",
    "outputId": "8ca4a0f1-85e7-4222-deeb-84b63514c28c"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_data_file)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpRmTUuvxrxc",
    "outputId": "09312305-ddf0-4eae-a480-7c082bd02687"
   },
   "outputs": [],
   "source": [
    "print('Unique comments in training: ', train_df.sentence.nunique() == train_df.shape[0])\n",
    "print('Null values in training: ', train_df.isnull().values.any())\n",
    "train_df = train_df.dropna()\n",
    "print('Null values after drop in training: ', train_df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edZccb-2lfBt",
    "outputId": "ada145e5-dd36-4b9b-94d8-920fd3ac0ca5"
   },
   "outputs": [],
   "source": [
    "train_df['label'] = pd.Categorical(train_df.score, ordered=False).codes\n",
    "train_df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "US-USWoale2_",
    "outputId": "baeff17a-0a88-4123-a85c-fd62232a3035"
   },
   "outputs": [],
   "source": [
    "mapLabels = pd.DataFrame(train_df.groupby(['score', 'label']).count())\n",
    "\n",
    "#drop count column\n",
    "mapLabels.drop(['sentence'], axis = 1, inplace = True)\n",
    "label2Index = mapLabels.to_dict(orient='index')\n",
    "\n",
    "print (f\"label2Index :{label2Index}\")\n",
    "print (type(label2Index))\n",
    "#print (f\"index2Label :{index2Label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jn-6gvAyuZcC",
    "outputId": "68ac16e6-ec9d-4869-ede9-c70a82ecd3c1"
   },
   "outputs": [],
   "source": [
    "index2label = {}\n",
    "\n",
    "for key in label2Index:\n",
    "  print (f\"{key[1]} -> {key[0]}\")\n",
    "  index2label[key[1]] = key[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TJeeIA0sx1kN",
    "outputId": "e4881ae3-de0c-408b-b934-fe2995e7386b"
   },
   "outputs": [],
   "source": [
    "label2Index = {v: k for k, v in index2label.items()}\n",
    "\n",
    "print (f'label2Index: {label2Index}')\n",
    "print (f'index2label: {index2label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "XwcbgaeYyb0d",
    "outputId": "c21ae39f-0e8b-414c-b71d-ecb7b67161e4"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4YNoaoDaK5v"
   },
   "outputs": [],
   "source": [
    "train_df.rename(columns = {'label' : 'LABEL_COLUMN', 'sentence' : 'DATA_COLUMN'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Crrzuh66bsgT"
   },
   "outputs": [],
   "source": [
    "# Remoe Email address to avoid additional noise\n",
    "train_df.DATA_COLUMN.replace(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', '', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WlfQrabQ0XJc"
   },
   "outputs": [],
   "source": [
    "train_df = train_df[['LABEL_COLUMN','DATA_COLUMN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "RMkmS0d-aKuX",
    "outputId": "f7613199-6e11-4088-f83a-9b2205d2964f"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l6Jm1uNuc-ek",
    "outputId": "8c729600-73e5-4611-c519-d824c9683e48"
   },
   "outputs": [],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-KRMQGfc-bE",
    "outputId": "0b83f9bf-e01a-4232-bc54-c76f9798242c"
   },
   "outputs": [],
   "source": [
    "#splitSize = df.count() * .8\n",
    "#splitSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5w5qeWTLc-XG"
   },
   "outputs": [],
   "source": [
    "#people_copy = people.copy()\n",
    "train = train_df.sample(frac=1, random_state=5)\n",
    "#new_data = train.sample(frac=0.8, random_state=0)\n",
    "\n",
    "#test = train_df.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3wtIxKBc-NR",
    "outputId": "7a90ae3e-f6a8-4fea-e262-794624377b98"
   },
   "outputs": [],
   "source": [
    "print (train.count())\n",
    "unique_labels = np.unique(train[\"LABEL_COLUMN\"].tolist())\n",
    "label_counts = train[\"LABEL_COLUMN\"].value_counts()\n",
    "print(label_counts)\n",
    "print(unique_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate the Gini Coefficient\n",
    "def gini_coefficient(array):\n",
    "    \"\"\"Calculate the Gini coefficient of a numpy array.\"\"\"\n",
    "    # All values are sorted and normalized (making the total equal to 1)\n",
    "    array = array / array.sum()\n",
    "    array = np.sort(array)\n",
    "    index = np.arange(1, array.shape[0] + 1)\n",
    "    n = array.shape[0]\n",
    "    return ((np.sum((2 * index - n - 1) * array)) / n)\n",
    "\n",
    "# Calculate the Gini Coefficient for the label counts\n",
    "gini = gini_coefficient(label_counts.values)\n",
    "print(f\"Gini Coefficient for the label distribution: {gini}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_file = '../dataset/extracted_files/'+dataset_name+'_test.csv'\n",
    "test_df = pd.read_csv(validation_data_file)\n",
    "test_df.head()\n",
    "print('Unique comments in testing: ', test_df.sentence.nunique() == test_df.shape[0])\n",
    "print('Null values in testing: ', test_df.isnull().values.any())\n",
    "test_df = test_df.dropna()\n",
    "print('Null values after drop in testing: ', test_df.isnull().values.any())\n",
    "test_df['score'] = pd.Categorical(test_df.score, ordered=True).codes\n",
    "test_df['score'].unique()\n",
    "test_df.rename(columns = {'score' : 'LABEL_COLUMN', 'sentence' : 'DATA_COLUMN'}, inplace = True)\n",
    "test_df.DATA_COLUMN.replace(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', '', regex=True, inplace=True)\n",
    "test_df = test_df[['LABEL_COLUMN','DATA_COLUMN']]\n",
    "test = test_df.sample(frac=1, random_state=5)\n",
    "print (test.count())\n",
    "#unique_labels = np.unique(test_data[\"LABEL_COLUMN\"].tolist())\n",
    "#label_counts = test_data[\"LABEL_COLUMN\"].value_counts()\n",
    "#print(label_counts)\n",
    "#print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qxCCr99vgJBx",
    "outputId": "5e471ba3-49c1-4822-a7a7-b47871d2ab5d"
   },
   "outputs": [],
   "source": [
    "uniqueLabels = train_df['LABEL_COLUMN'].unique()\n",
    "print (f'Number of Labels: {len(uniqueLabels)},\\nLabels:{uniqueLabels}')\n",
    "sentences = list(train_df.DATA_COLUMN.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrXQ6Tw1fuBF"
   },
   "source": [
    "## Load the Model\n",
    "See Load and Save notebooks in this repository to understand how Transformers models cen be:\n",
    "1. Downloaded\n",
    "2. Stored Locally and\n",
    "3. be used from Local Storage.\n",
    "\n",
    "This should be interesting if you work in a cloud environment without Internet connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPqJYYaTEeA2"
   },
   "source": [
    "Here we tell the model that we whish to train on **20 label values** instead of the original 1 label (with 1 or 0 values) for which the original model was designed. This is why the test below tells us that we better should train this model. So, training it we will :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdCpOa3CvZjo",
    "outputId": "cfdd8879-0ff9-4f2c-a3f5-e3291293733d"
   },
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-german-cased', num_labels=len(uniqueLabels))\n",
    "#model = TFBertForSequenceClassification.from_pretrained('../models/G-SciEdBert', from_pt=True, num_labels=len(uniqueLabels))\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased', do_lower_case=True) # tokenizer\n",
    "encodings = tokenizer.batch_encode_plus(sentences,max_length=max_length,pad_to_max_length=True) # tokenizer's encoding method\n",
    "print('tokenizer outputs: ', encodings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "omoxwyaEaH5R",
    "outputId": "42da7204-4083-49db-a3a9-3f46aeef44e5"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OY__mmNzbFPs"
   },
   "source": [
    "## Creating Input Sequences\n",
    "We have two pandas Dataframe objects waiting for us to convert them into suitable objects for the BERT model. We will take advantage of the InputExample function that helps us to create sequences from our dataset. The InputExample function can be called as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YgcKjdFIbCAL",
    "outputId": "36ccb947-f433-442a-ebaa-f4ce69ce9070"
   },
   "outputs": [],
   "source": [
    "# transformers.InputExample\n",
    "InputExample(guid=None,\n",
    "             text_a = \"Hello, world\",\n",
    "             text_b = None,\n",
    "             label = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbU6diuYbNVM"
   },
   "source": [
    "Now we will create two main functions:\n",
    "\n",
    "1 — `convert_data_to_examples`: This will accept our train and test datasets and convert each row into an InputExample object.\n",
    "\n",
    "2 — `convert_examples_to_tf_dataset`: This function will tokenize the InputExample objects, then create the required input format with the tokenized objects, finally, create an input dataset that we can feed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZobDj7ZibI78"
   },
   "outputs": [],
   "source": [
    "def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN):\n",
    "  train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN],\n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "  validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN],\n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "  return train_InputExamples, validation_InputExamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wUBIWuEbleM"
   },
   "outputs": [],
   "source": [
    "train_InputExamples, validation_InputExamples = convert_data_to_examples(train,\n",
    "                                                                           test,\n",
    "                                                                           'DATA_COLUMN',\n",
    "                                                                           'LABEL_COLUMN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k52dErBYbVst"
   },
   "outputs": [],
   "source": [
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in examples:\n",
    "        # Documentation is really strong for this method, so please take a look at it\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length, # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n44iQfkQba9M"
   },
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'DATA_COLUMN'\n",
    "LABEL_COLUMN = 'LABEL_COLUMN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JsahLWQIfsMv",
    "outputId": "efbd9ade-e05a-4fd2-c134-f1571cfa78dd"
   },
   "outputs": [],
   "source": [
    "print (str(type(DATA_COLUMN)) + ' ' + DATA_COLUMN)\n",
    "print (str(type(LABEL_COLUMN)) + ' ' + LABEL_COLUMN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "GzSyFUoKgbNE",
    "outputId": "71544918-3125-4fa9-d27b-58e349879d27"
   },
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BR_puwpBbtid",
    "outputId": "d05cec17-0bbb-41a5-d06e-ee097eb3c0ec"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_InputExamples, validation_InputExamples = convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN)\n",
    "\n",
    "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
    "\n",
    "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "validation_data = validation_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (test.count())\n",
    "unique_labels = np.unique(test[\"LABEL_COLUMN\"].tolist())\n",
    "label_counts = test[\"LABEL_COLUMN\"].value_counts()\n",
    "print(label_counts)\n",
    "print(unique_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate the Gini Coefficient\n",
    "def gini_coefficient(array):\n",
    "    \"\"\"Calculate the Gini coefficient of a numpy array.\"\"\"\n",
    "    # All values are sorted and normalized (making the total equal to 1)\n",
    "    array = array / array.sum()\n",
    "    array = np.sort(array)\n",
    "    index = np.arange(1, array.shape[0] + 1)\n",
    "    n = array.shape[0]\n",
    "    return ((np.sum((2 * index - n - 1) * array)) / n)\n",
    "\n",
    "# Calculate the Gini Coefficient for the label counts\n",
    "gini = gini_coefficient(label_counts.values)\n",
    "print(f\"Gini Coefficient for the label distribution: {gini}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qe-9PgI_ckZv"
   },
   "source": [
    "## Configuring the BERT model and Fine-tuning\n",
    "We will use Adam as our optimizer, CategoricalCrossentropy as our loss function, and SparseCategoricalAccuracy as our accuracy metric. Fine-tuning the model for 2 epochs will give us good accuracy, which is great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bxypaKh8cg3m",
    "outputId": "fbc15683-fdd8-4ecb-f393-b6b27ed93752",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n",
    "model.fit(train_data, epochs=2, validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BQ1f4sbszxJV",
    "outputId": "545aecf0-e8c0-49f0-bcb6-767793eb21f5"
   },
   "outputs": [],
   "source": [
    "#torch.save(model,output_model_name)\n",
    "model.save_pretrained(output_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_file = '../dataset/extracted_files/'+dataset_name+'_test.csv'\n",
    "test_df = pd.read_csv(validation_data_file)\n",
    "test_df.head()\n",
    "print('Unique comments in testing: ', test_df.sentence.nunique() == test_df.shape[0])\n",
    "print('Null values in testing: ', test_df.isnull().values.any())\n",
    "test_df = test_df.dropna()\n",
    "print('Null values after drop in testing: ', test_df.isnull().values.any())\n",
    "test_df['score'] = pd.Categorical(test_df.score, ordered=True).codes\n",
    "test_df['score'].unique()\n",
    "test_df.rename(columns = {'score' : 'LABEL_COLUMN', 'sentence' : 'DATA_COLUMN'}, inplace = True)\n",
    "test_df.DATA_COLUMN.replace(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', '', regex=True, inplace=True)\n",
    "test_df = test_df[['LABEL_COLUMN','DATA_COLUMN']]\n",
    "test_data = test_df.sample(frac=1, random_state=5)\n",
    "print (test_data.count())\n",
    "unique_labels = np.unique(test_data[\"LABEL_COLUMN\"].tolist())\n",
    "label_counts = test_data[\"LABEL_COLUMN\"].value_counts()\n",
    "print(label_counts)\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sentences= test_data[\"DATA_COLUMN\"].tolist()\n",
    "validation_labels = test_data[\"LABEL_COLUMN\"].tolist()\n",
    "tf_batch = tokenizer(pred_sentences, max_length=512, padding=True, truncation=True, return_tensors='tf')\n",
    "tf_outputs = model(tf_batch)\n",
    "tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "\n",
    "# Get index of predicted label for each sentence\n",
    "predicted_labels = tf.argmax(tf_predictions, axis=1).numpy()\n",
    "\n",
    "true_positives = 0\n",
    "\n",
    "# output human readable label predictions\n",
    "for i in range(len(pred_sentences)):\n",
    "    predicted_label = predicted_labels[i]\n",
    "    actual_label = validation_labels[i]\n",
    "    if predicted_label == actual_label:\n",
    "        true_positives+=1\n",
    "accuracy = true_positives/len(pred_sentences)\n",
    "print(\"Overall testing Accuracy:\",accuracy )\n",
    "        \n",
    "\n",
    "    \n",
    "#for i in range(len(pred_sentences)):\n",
    "    #print(pred_sentences[i], \": \\n\", str(predicted_labels[i]) +\" with score: \"+ str(tf_predictions[i][predicted_labels[i]].numpy()))\n",
    "    #print (\"Actual Label:\",str(validation_labels[i]) )\n",
    "\n",
    "# Compute accuracy for each label\n",
    "unique_labels = np.unique(validation_labels)\n",
    "label_accuracies = {}\n",
    "\n",
    "for label in unique_labels:\n",
    "    correct_predictions = np.sum((predicted_labels == label) & (validation_labels == label))\n",
    "    total_label_count = np.sum(validation_labels == label)\n",
    "    \n",
    "    accuracy = correct_predictions / total_label_count\n",
    "    label_accuracies[label] = accuracy\n",
    "\n",
    "print(\"Validation accuracy for each label:\", label_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "M7--kUGo034F",
    "outputId": "508d8274-c2ac-42b3-e664-a69e45bb6ef9"
   },
   "outputs": [],
   "source": [
    "#model = torch.load(output_model_name)\n",
    "model_name = 'gelatin_gender2'\n",
    "output_model_name = '../models/bert_model_'+model_name\n",
    "#output_model_name = '../models/bert_model_ETS_CH_gelatin'\n",
    "new_model = TFBertForSequenceClassification.from_pretrained(output_model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) # tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pS9SoNm-dD07"
   },
   "source": [
    "Training the model might take a while, so ensure you enabled the GPU acceleration from the Notebook Settings. After our training is completed, we can move onto making sentiment predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2kIo1BtdOai"
   },
   "source": [
    "## Making Predictions\n",
    "I created a list of two reviews I created. The first one is a positive review, while the second one is clearly negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2HLb5gFcpL-"
   },
   "outputs": [],
   "source": [
    "pred_sentences = [\"The water is only stirring while the weight is falling. When the weight falls, the paddle will stop stirring.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'gelatin'\n",
    "sub_dataset = 'gender1'\n",
    "data_file = '../datasets/'+dataset_name+'/'+sub_dataset+'_test.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "df.head()\n",
    "print('Unique comments in training: ', df.sentence.nunique() == df.shape[0])\n",
    "print('Null values in training: ', df.isnull().values.any())\n",
    "df = df.dropna()\n",
    "print('Null values after drop in training: ', df.isnull().values.any())\n",
    "pred_sentences = list(df['sentence'])\n",
    "actual_labels = list(df['score'])\n",
    "print(len(pred_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rM2YzQrmdXUa"
   },
   "source": [
    "We need to tokenize our reviews with our pre-trained BERT tokenizer. We will then feed these tokenized sequences to our model and run a final softmax layer to get the predictions. We can then use the argmax function to determine whether our sentiment prediction for the review is positive or negative. Finally, we will print out the results with a simple for loop. The following lines do all of these said operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lxD4gyExdTZ6",
    "outputId": "ef8d5fdd-1247-44a7-c4c9-cc75f5880517"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')\n",
    "tf_outputs = new_model(tf_batch)\n",
    "tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "\n",
    "# Get index of predicted label for each sentence\n",
    "pred_label = tf.argmax(tf_predictions, axis=1).numpy()\n",
    "num_classes = tf_predictions.shape[1]\n",
    "\n",
    "# output human readable label predictions\n",
    "for i in range(len(pred_sentences)):\n",
    "    print(pred_sentences[i], \": \\n\", str(pred_label[i]) +\" with score: \"+ str(tf_predictions[i][pred_label[i]].numpy()))\n",
    "  #print(pred_sentences[i], \": \\n\", str(index2label[label[i]]) +\" with score: \"+ str(tf_predictions[i][label[i]].numpy()))\n",
    "    print ()\n",
    "with open('../outputfiles/'+model_name+'Model_'+sub_dataset+'_w_all_probs.csv', 'w',encoding=\"utf-8\", newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    # Writing headers\n",
    "    headers = ['Sentence', 'Actual Score', 'Predicted Score', 'Predicted Score Probability']\n",
    "    headers += [f'Probability_Score_{i}' for i in range(num_classes)]\n",
    "    csvwriter.writerow(headers)\n",
    "\n",
    "    # Write data\n",
    "    for i in range(len(pred_sentences)):\n",
    "        sentence = pred_sentences[i]\n",
    "        actual_score = actual_labels[i]  # or any other method to obtain the actual score\n",
    "        bert_score = pred_label[i]\n",
    "        probability = tf_predictions[i][pred_label[i]].numpy()\n",
    "        probabilities = tf_predictions[i].numpy().tolist()\n",
    "\n",
    "        # Write the row to the CSV file\n",
    "        csvwriter.writerow([sentence, actual_score, bert_score, probability] + probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')\n",
    "tf_outputs = new_model(tf_batch)\n",
    "tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "tf_predictions\n",
    "tf.argmax(tf_predictions, axis=1).numpy()\n",
    "index2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUd8tmWUnK28"
   },
   "source": [
    "## Debugging the Final Tensor Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IDRGxQh_V8bt",
    "outputId": "5182dad6-03ad-4915-83d3-d40bbc9d4b3c"
   },
   "outputs": [],
   "source": [
    "tf_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CRDxUOJmgvK4",
    "outputId": "d3d5c68a-3707-4af3-a049-e4455c4e1c5e"
   },
   "outputs": [],
   "source": [
    "for i in range(len(tf_predictions)):\n",
    "  print (tf_predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cjcFubhmhFQB",
    "outputId": "b591d960-32ee-406c-9149-2c761a2e01f1"
   },
   "outputs": [],
   "source": [
    "for i in range(len(tf_predictions)):\n",
    "  print (str(tf_predictions[i][0]) + ' - ' + str(tf_predictions[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q88Ppt5Dh9yb",
    "outputId": "96c35101-ce0a-4141-fcef-8220f344e7b4"
   },
   "outputs": [],
   "source": [
    "for i in range(len(tf_predictions)):\n",
    "  print(tf_predictions[i][label[i]].numpy())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
